# -*- coding: utf-8 -*-
"""AI_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mn_piBfp2gqGZ_SVMVunUyzPX-HJYiL4

##DIAGNOSIS OF HEART DISEASE USING MACHINE LEARNING

Heart disease is the leading cause of death in the United States, causing about 1 in 4 deaths,i.e., almost 25% of the deaths. Heart disease depends on various factors such as presence of colesterol, blood pressure, age, weight, alcohol consumption etc.
It is said that an experianced physician can able to predict the heart disease around 67%, and most the cases the physicians overestimated the prediction.
Using machine learning models the prediction is around 73% and also the precision value is 74%.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns # used for plot interactive graph.
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import average_precision_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_recall_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_curve
from sklearn.metrics import f1_score
from sklearn.metrics import auc
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
# %matplotlib inline

"""#Importing File

I used the dataset from the kaggle and used no data cleaning as this presentation is only for academic purpose.
https://www.kaggle.com/sulianova/cardiovascular-disease-dataset
"""

from google.colab import files
uploaded = files.upload()

"""#Read the File"""

df = pd.read_csv('cardio_train.csv.zip',delimiter=';')
df.head(20)

df.shape

"""There are 13 features columns and 70000 samples in the dataset.Here the target feature is 'cardio'."""

# transform data to numeric to enable further analysis
#df = df.apply(pd.to_numeric)
df.dtypes

df.isna().sum()

df['cardio'].value_counts()

df['years']=(df['age']/365).round(0)
df['years']=pd.to_numeric(df['years'],downcast='integer')

"""I created a column 'years' in the data set."""

df.head()

"""Then I ploted the number of cardio positive and cardio negative datas according to different age group."""

plt.figure(figsize=(16,10))
sns.countplot(x='years',hue='cardio',data=df,palette='colorblind',edgecolor=sns.color_palette('dark',n_colors=1))

"""#CORRELATION MATRIX

Using correlation matrix I am abled to find the correlation of different features with the target feature 'cardio'.
"""

df.corr()

"""#HEATMAP OF THE CORRELATION MATRIX"""

plt.figure(figsize=(10,10))
sns.heatmap(df.corr(),annot=True,fmt='.0%')

sns.set_style('whitegrid')
sns.countplot(x='cardio',data=df,palette='RdBu_r')

df=df.drop('years',axis=1)

df=df.drop('id',axis=1)

"""# TRAIN-TEST SPLIT OF THE DATA

Now I splited the dataset into training and testing set with training set as 75%and testing set as 25%.
"""

for column in df.columns:
    if df[column].dtype == type(object):
        le = sklearn.preprocessing.LabelEncoder()
        df[column] = le.fit_transform(df[column])
X = df.drop(columns='cardio')
y = df['cardio']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5, stratify=y)

"""#PRE-PROCESSING OF THE DATA USING STANDARD SCALER

Before applying any machine learning model I used standard scaler method for the preprocessing of the dataset. So that the data lies between 0 and 1.
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Here I used four machine learning algorithm:

1.Decision Tree Classifier
2.Random Forest Classifier
3.XGBoosting Classifier
4.Logistic Regression
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
import sklearn.metrics as metrics

"""Now I used Decision tree as first machine learning model.

#DecisionTreeClassifier
"""

#predicting
tree = DecisionTreeClassifier(random_state=5)
tree.fit(X_train, y_train)
y_pred_tree=tree.predict(X_test)
#confusion matrix for both data
cnf_tree= metrics.confusion_matrix(y_test, y_pred_tree)
cnf_tree

"""For the evaluation method, we used confusion matrix.

Accuracy is how accurate the model is.
Precision is how precise the model is, i.e.,how often the model is correct. Recall is the sensitivity of a model,i.e., how often model can identify the test data. And F1 score is very important for a dataset which is uneven,i.e., when the dataset contains high values and low values.
"""

#Accurary , pre and recall for tree
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_tree))
print("Precision:",metrics.precision_score(y_test, y_pred_tree))
print("Recall:",metrics.recall_score(y_test, y_pred_tree))
print("F1 score:",metrics.f1_score(y_test,y_pred_tree))

"""## Optimising hyperparameter of a decision tree model using Grid search

To enhance the accuracy of the model we used grid search cv. And applying grid search cv I am abled to increase the accuracy score.
"""

def dtree_optimaization():
    print()
    
    import warnings
    warnings.filterwarnings("ignore")

    # load libraries
    from sklearn import decomposition, datasets
    from sklearn import tree
    from sklearn.model_selection import GridSearchCV, cross_val_score
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    # scaler object creation
    sc = StandardScaler()

    #pca object creation
    pca = decomposition.PCA()

    # Create a decision tree object
    decisiontree = tree.DecisionTreeClassifier()

    # Create a dictionary 
    #First element with standardising data
    # Second, tranforming the data with PCA method
    # last decision tree training
    pipe = Pipeline(steps=[('sc', sc),
                           ('pca', pca),
                           ('decisiontree', decisiontree)])

    # Create n components 
    n_components = list(range(1,X_train.shape[1]+1,1))


    # Create a dictionary of all the parameter options 
    parameters = dict(pca__n_components=n_components,
                      decisiontree__criterion=['gini', 'entropy'],
                      decisiontree__max_depth= [4,6,8,12])

    # Creating a grid search object
    clf = GridSearchCV(pipe, parameters)

    # Fitting the grid
    clf.fit(X_train, y_train)

    # View The optimised Parameters
    print('Optimised Criterion:', clf.best_estimator_.get_params()['decisiontree__criterion'])
    print('Optimised max_depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])
    print('Optimised Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])
    print(); print(clf.best_estimator_.get_params()['decisiontree'])

    # Use Cross Validation To Evaluate Model
    Result =  cross_val_score(clf, X_train, y_train, cv=4, n_jobs=-1)
    print(); print(Result)
    #mean
    print(); print(Result.mean())
    #std
    print(); print(Result.std())

dtree_optimaization()

"""Second model I used is Random forest classifier and we all know that it is an ensemble method and very robust.Here also I used confusion matrix for the evaluation of the metrics.

##Random Forest Classifier
"""

rf= RandomForestClassifier(n_estimators=10)
rf.fit(X_train,y_train)

y_pred_rf=rf.predict(X_test)

#confusion matrix for both data
cnf_rf= metrics.confusion_matrix(y_test, y_pred_rf)
cnf_rf

#Accurary , pre , recall and F1 score for rf
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_rf))
print("Precision:",metrics.precision_score(y_test, y_pred_rf))
print("Recall:",metrics.recall_score(y_test, y_pred_rf))
print("F1 score:",metrics.f1_score(y_test,y_pred_rf))
print(classification_report(y_test,y_pred_rf))

"""In Random forest classifier I got better result than decision tree classifier.

##XGBoosting Classifier

XGBoosting is a gradient boosted decision tree based ensemble machine learning algorithm designed for speed and performance.
"""

#XGBClassifier
model=xgb.XGBClassifier(random_state=5,learning_rate=0.01)
model.fit(X_train, y_train)
model.score(X_test,y_test)

"""#XGBoostingClassifier optimization"""

#XGBClassifier optimization
from sklearn.model_selection import GridSearchCV
xgb_model=xgb.XGBClassifier(random_state=5,learning_rate=0.01)
optimization_dict = {'max_depth': [2,4,6],
                     'n_estimators': [50,100,200]}
model1 = GridSearchCV(xgb_model, optimization_dict,
                     scoring='accuracy', verbose=1)
model1.fit(X_train, y_train)
print(model1.best_score_)
print(model1.best_params_)

"""#LogisticRegression

The fourth model I used is Logistic regression which is used when the target variable is categorical.
"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train,y_train)

#predicting
y_pred_LR=logreg.predict(X_test)

#confusion matrix for both data
cnf_LR= metrics.confusion_matrix(y_test, y_pred_LR)
cnf_LR

#Accurary , pre , recall and F1 score for LR
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_LR))
print("Precision:",metrics.precision_score(y_test, y_pred_LR))
print("Recall:",metrics.recall_score(y_test, y_pred_LR))
print("F1 Score:",metrics.f1_score(y_test, y_pred_LR))

"""#Improving Logistic Regression

I tried to increase the accuracy of the model, but unfortunately the accuracy and precision both decreased.
"""

min_value_test=X_test.min()
range_test=(X_test-min_value_test).max()
X_test_scale=(X_test-min_value_test)/range_test

#predicting
y_pred_LR1=logreg.predict(X_test_scale)

#confusion matrix for both data
cnf_LR1= metrics.confusion_matrix(y_test, y_pred_LR1)
cnf_LR1

#Accurary , pre, recall and F1 score for LR
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_LR1))
print("Precision:",metrics.precision_score(y_test, y_pred_LR1))
print("Recall:",metrics.recall_score(y_test, y_pred_LR1))
print("F1 Score:",metrics.f1_score(y_test, y_pred_LR1))

"""So comparing all the four models, it is observed that all the models have accuracy score and precision value more than 70%. XGBoosting has model score around 73%. XGBoosting and Logistic Regression gave best results.

For future I can enhance the model accuracy score more than 80%. I can use different dataset and optimize the accuracy score and precision value more than 80%.
"""





